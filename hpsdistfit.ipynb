{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# About this Notebook\n",
    "\n",
    "This Jupyter Notebook can be used to replicate the computational experiments and results presented in the following paper\n",
    "\n",
    "> Paradis, Gregory E. 2019. _A simplified method for estimating stem diameter distributions from horizontal point sample data_. Forestry (submitted).\n",
    "\n",
    "The code in this notebook is implemented using freely-available open-source software libraries, and should be run using a Python 3 kernel. The notebook can be downloaded from [GitHub](https://github.com/gparadis/hpsdistfit).\n",
    "\n",
    "# Background\n",
    "\n",
    "Diameter frequency distributions are a key piece of information describing forest stands, for both practical forestry applications and scientific research on forest ecosystems. A substantial fraction of forest inventory data is collected using horizontal point sampling (HPS) methods. The procedure for deriving diameter distributions from HPS data is clearly described in the literature (e.g., see Ducey and Gove, 2015), but is somewhat complex.\n",
    "\n",
    "Paradis (2019) presents a simplified method for estimating stem diameter distributions from horizontal point sample data, including a computational experiment showing that output from our method is essentially identical to the more complex reference method. We hope our method will make it easier for practitioners and researchers alike to correctly derive diameter distributions from HPS datasets.\n",
    "\n",
    "This notebook contains instructions and code that can be used to replicate the computational experiment and results presented in Paradis (2019), as well as constitute a known-working software implementation of the simplified method presented therein.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "Load required packages, define global variables, and define some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, rcParams\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "import math\n",
    "import scipy.version\n",
    "from scipy import integrate\n",
    "from scipy.special import gamma, beta, betaln\n",
    "from scipy import stats \n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from scipy.optimize import curve_fit as _curve_fit\n",
    "import ipy_table as ipt\n",
    "import pysal as ps\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from scipy.stats.distributions import t\n",
    "from numpy import zeros, arange\n",
    "from math import sqrt\n",
    "import pickle\n",
    "from functools import partial, wraps\n",
    "import types\n",
    "from lmfit import Model, Parameter, MinimizerException\n",
    "from lmfit.model import ModelResult\n",
    "\n",
    "pd.set_option('display.max_columns', 10)\n",
    "sns.set_context('paper', font_scale=1.0)\n",
    "sns.set_style('white')\n",
    "#rc('text',usetex=True)\n",
    "#rc('font',**{'family':'serif','serif':['Computer Modern']})\n",
    "\n",
    "data_path = './dat/'\n",
    "size_classes = range(10, 62, 2) # 2 cm DBH classes in range [10, 60]\n",
    "treatment_types = [1, 2, 3] # 1: clearcut, 2: selection cut, 3: commercial thin\n",
    "cover_types = ['r', 'm', 'f'] # r: softwood, m: mixedwood, f: hardwood\n",
    "species_group_names = {'auf':'Other Hardwoods',\n",
    "                       'boj':'Yellow Birch',\n",
    "                       'bop':'White Birch',\n",
    "                       'chce':'Oak-Hickory',\n",
    "                       'ers':'Sugar Maple',\n",
    "                       'erx':'Other Maples',\n",
    "                       'peu':'Poplar',\n",
    "                       'pib':'Eastern White Pine',\n",
    "                       'pir':'Red Pine',\n",
    "                       'sepm':'Fir-Spruce-Pine-Larch',\n",
    "                       'topu':'Eastern White Cedar'}\n",
    "cover_type_names = {'r':'Softwood', \n",
    "                    'm':'Mixedwood',\n",
    "                    'f':'Hardwood'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions that will be used further down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `scipy.optimize.curve_fit`, which can handle a wide range of curve fitting tasks. By default, `curve_fit` calls `scipy.optimize.leastsq` for unbounded problems, which has an optional `maxfev` argument (related to the number of iterations the algorithm will run before giving up and declaring non-convergence) that we need to set to a big number (`curve_fit` throws too many errors with default values). For bounded problems, `curve_fit` calls `scipy.optimize.least_squares`, which has similar optional argument named `max_nfev`. Depending on whether we set bounds on parameter values or not, we need to call `curve_fit` with different keyword argument names (either `max_nfev` or `maxfev`). Therefore, we have to test for boundedness of problem before every call to `curve_fit` to make sure we pass the correct keyword argument name. Yuck. Perhaps `scipy` package maintainers will patch the interface someday to make this work as expected.  \n",
    "\n",
    "Meanwhile, we can get around all this nonsense by defining a wrapper function for `scipy.optimize.curve_fit` that tests problem boundedness and renames `maxfev` keyword argument to `max_nfev` if the problem is bounded. If called via this wrapper, `curve_fit` now seems to behaves the way it should (i.e. according to the POLA). \n",
    "\n",
    "Note that this problem seems to have been [patched](https://github.com/scipy/scipy/commit/eef17945bf0f89344ea9c1ce8ce2ec33f0e401f8) in the `master` branch on GitHub since the latest release of `scipy` (`0.18.1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@wraps(_curve_fit)\n",
    "def curve_fit(*args, **kwargs):\n",
    "    b = kwargs['bounds'] if 'bounds' in kwargs else None \n",
    "    if b and np.any(np.isfinite(b)) and 'max_nfev' not in kwargs:\n",
    "        kwargs['max_nfev'] = kwargs.pop('maxfev', None)\n",
    "    return _curve_fit(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function that standardizes compilation of a bin centers, bin values, bin edges, and bin standard errors from a `pandas.DataFrame` containing our raw PSP inventory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def compile_bin_data(data, bins=None, xminmm=90, xmaxmm=610, xwmm=20, ef=25., alpha=0.05, pid_colname='id_pep', dbh_colname='dhpmm', normed=True, compile_stderr=False):\n",
    "    #display(data)\n",
    "    df = data.reset_index().set_index(pid_colname)\n",
    "    if bins is None:\n",
    "        bins = np.arange(xminmm, xmaxmm+xwmm, xwmm)\n",
    "\n",
    "    bin_vals, bin_edges = np.histogram(df[dbh_colname], bins=bins, range=(xminmm, xmaxmm))\n",
    "    bin_vals = bin_vals * ef # expand to stems/ha\n",
    "    bin_stderrs = []\n",
    "    if compile_stderr:\n",
    "        for i, (xa, xb) in enumerate([(x, x+xwmm) for x in bins[:-1]]):\n",
    "            #print 'processing bin', i\n",
    "            Y = []\n",
    "            for pid in df.index.unique():\n",
    "                #print i, pid, len(df.index.unique())\n",
    "                try:\n",
    "                    _df = df.loc[[pid]].query('%s > %i & %s <= %i' % (dbh_colname, xa, dbh_colname, xb))\n",
    "                except:\n",
    "                    print(df.loc[pid]) \n",
    "                    raise\n",
    "                Y.append(len(_df) * ef)\n",
    "            sigma = sqrt((sum(y**2 for y in Y) - sum(Y)/len(Y))/(len(Y) - 1))\n",
    "            t = distributions.t.ppf(1. - alpha/2., len(Y) - 1) \n",
    "            #print t*sigma\n",
    "            bin_stderrs.append(t * sigma)\n",
    "    else:\n",
    "        bin_stderrs = [0. for i in bin_vals]\n",
    "    bin_stderrs = np.array(bin_stderrs)\n",
    "    bin_centers = (bin_edges[:-1] + (xwmm * 0.5)) * 0.1\n",
    "    if normed: \n",
    "        scale = float(0.1 * xwmm * sum(bin_vals))\n",
    "        bin_vals = bin_vals * pow(scale, -1)\n",
    "        bin_stderrs = bin_stderrs * pow(scale, -1)\n",
    "    return bin_centers, bin_vals, bin_edges * 0.1, bin_stderrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Inventory Data\n",
    "\n",
    "The computational experiment implemented in this notebook uses permanent sample data collected in Quebec (Canada). Detailed information on the Quebec PSP inventory program under which our test data was collected is available from the [Ministère des forêts, faune, et parcs (MFFP)](http://mffp.gouv.qc.ca/les-forets/inventaire-ecoforestier/), including technical documentation on inventory methods, data standards, and contact information. The full PSP dataset can be downloaded from [Données Québec](ftp://transfert.mffp.gouv.qc.ca/Public/Diffusion/DonneeGratuite/Foret/DONNEES_FOR_ECO_SUD/Placettes_permanentes/PEP_GDB.zip). \n",
    "\n",
    "We preprocessed the full PSP dataset to include only live, merchantable stems from the fourth decennial inventory cycle, from the largest of 8 plot networks, corresponding to mature, undisturbed stands, for which there was valid data in all fields. This notebook loads a pickle file (`dat/misc/tiges_final_full.p`) that contains a serialized `pandas.DataFrame` object created in the preprocessing step. We include filtered dataset here for convenience, and to facilitate reproduction of results presented in Paradis (2019). It is advisable that someone with expert understanding of the statistical limitation of Quebec PSP data be tasked with re-filtering the PSP dataset on a per-project basis, to ensure that the subset of data used best match the needs and goals of each project. \n",
    "\n",
    "The data preprocessing method is described an implemented in a separate notebook (`psp_preprocess.ipynb`). The PSP database we used as input for the processing step is packaged differently thant the freely-available PSP data downloadable using the link above (i.e., several DBF files that implicitly form a relational database, versus a single MDB file in the downloadable dataset). The underlying data is the same, but some work is required at the time of writing this to finish adapting the preprocessing notebook code to use the newer MDB database as input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define statistical distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalized beta family of statistical distributions is useful for modelling stem density distributions. All members of the family can be derived from either the generalized beta distribution of the first kind (GB1) or the generalized beta distribution of the second kind (GB2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalized gamma GG distribution is a special case of both GB1 and GB2 distributions. The GG PDF has the following form\n",
    "\n",
    "$$\n",
    "\\text{GG}(x; a, b, p) = \\frac{ax^{ap-1}e^{-\\left(\\frac{x}{b}\\right)^a}}{b^{ap}\\Gamma(p)}, \\qquad a > 0, b > 0, q > 0\n",
    "$$\n",
    "\n",
    "defined for $x > 0$, where $\\Gamma(p)$ represents the [gamma function](https://en.wikipedia.org/wiki/Gamma_function) (not to be confounded with the gamma, or generalized gamma, distributions), which is given by\n",
    "\n",
    "$$\n",
    "\\Gamma(p) = \\int_0^\\infty x^{p-1}e^{-x} dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size-biased form of the GG PDF is given by (adapted from Ducey and Gove, 2015)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{GG}_{\\text{SB}}(x; a, b, p, \\alpha) &= \\text{GG}(x; a, b, p + \\alpha/a)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define standard and size-biased forms of the PDFs of Weibull and Gamma distributions in terms GG PDFs.\n",
    "\n",
    "The standard forms are given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{W}(x; a, b) &= \\text{GG}(x; a, b, 1) \\\\\n",
    "\\text{GA}(x; b, p) &= \\text{GG}(x; 1, b, p) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The size-biased forms are given by (adapted from Ducey and Gove, 2015)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{W}_{\\text{SB}}(x; a, b, \\alpha) &= \\text{GG}(x; a, b, 1 + \\alpha/a),& \\qquad \\alpha > -a \\\\\n",
    "\\text{GA}_{\\text{SB}}(x; b, p, \\alpha) &= \\text{GG}(x; 1, b, p + \\alpha),& \\qquad \\alpha > -p \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement standard and size-biased forms of GG PDFs as follows. Note that we add a global scaling parameter $s$ to the implemented functions, to allow the fitting algorithm to compensate for the truncated sample domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gg_pdf(x, a, b, p, s=1.):\n",
    "    y = s * ((a * pow(x, a*p-1.) * np.exp(-pow(x/b, a))) / (pow(b, a*p) * gamma(p)))\n",
    "    return 0. if (np.any(np.isnan(y)) or np.any(np.isinf(y))) else y\n",
    "\n",
    "def gg_sb_pdf(x, a, b, p, s, alpha):\n",
    "    return gg_pdf(x, a, b, p+alpha/a, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement standard and size-biased forms of the Weibull and Gamma distributions by wrapping `gg_pdf` and `gg_sb_pdf`. For convenience later, store references to these distribution functions in a dictionary `D`, indexed on type and distribution. Note that we also define `sb2` type distributions, which are size-biased forms with the `alpha` parameter fixed at a value of 2 (i.e., we need second-order distibutions to model HPS tally data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim0 = pow(np.nan_to_num(np.inf), -1)\n",
    "\n",
    "D = {'st':{}, 'sb':{}, 'sb2':{}}\n",
    "\n",
    "# standard forms\n",
    "D['st']['gg'] = gg_pdf\n",
    "D['st']['ga'] = lambda x, beta, p, s: gg_pdf(x, 1., beta, p, s)\n",
    "D['st']['w'] = lambda x, a, beta, s: gg_pdf(x, a, beta, 1., s)\n",
    "\n",
    "# size-biased forms\n",
    "D['sb']['gg'] = gg_sb_pdf\n",
    "D['sb']['ga'] = lambda x, beta, p, s, alpha: gg_pdf(x, 1., beta, p + alpha, s)\n",
    "D['sb']['w'] = lambda x, a, beta, s, alpha: gg_pdf(x, a, beta, 1. + alpha/a, s)\n",
    "\n",
    "# size-biased forms with alpha fixed at 2. \n",
    "# [Is there a better way to freeze a parameter and remove it from the signature?]\n",
    "D['sb2']['gg'] = lambda x, a, beta, p, s: D['sb']['gg'](x, a, beta, p, s, 2.)\n",
    "D['sb2']['ga'] = lambda x, beta, p, s: D['sb']['ga'](x, beta, p, s, 2.)\n",
    "D['sb2']['w'] = lambda x, a, beta, s: D['sb']['w'](x, a, beta, s, 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a stand table from HPS data, one must multiply each stem observation by an _expansion factor_ $f_E(x, C_{BA})$, which is related to the DBH $x$ (measured in cm) and the BAF $C_{BA}$ as follows.\n",
    "\n",
    "$$\n",
    "f_E(x, C_{BA}) = \\frac{40000C_{BA}}{\\pi x^2} \n",
    "$$\n",
    "\n",
    "Stand table data can be converted back to HPS tally data by multiplying stand table data by a _compression factor_ $f_C(x; C_{BA})$, which is simply the multiplicative inverse of the expansion function, this is\n",
    "\n",
    "$$\n",
    "f_C(x; C_{BA}) = (f_E(x; C_{BA}))^{-1} = \\frac{\\pi x^2}{40000C_{BA}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper functions to expand or compress data (we will be bouncing back and forth between tally and stand table space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def expansion_factor(x, baf=2.):\n",
    "    \"\"\"Return stand table expansion factor for given DBH and BAF.\"\"\"\n",
    "    return baf / (math.pi * pow(x * 0.01 * 0.5, 2))\n",
    "\n",
    "def compression_factor(x, expansion_func=expansion_factor):\n",
    "    \"\"\"Return the multiplicative inverse of an expansion function.\"\"\"\n",
    "    return pow(expansion_func(x), -1)\n",
    "\n",
    "def transform_data(xdata, ydata, trans_func, normed=False):\n",
    "    _e = np.vectorize(trans_func)\n",
    "    result = _e(xdata) * ydata\n",
    "    if normed:\n",
    "        return result / sum(result)\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load some inventory data (pre-processed, see note at top of notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23233"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pickle.load(open(data_path+'misc/tiges_final_full.p', 'rb'), encoding='latin1')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was compiled from PEP inventory data, which is collected using a 11.28 m radius fixed-area plot sampling (FAPS) methodology. Thus, the expansion factor for all stems in this dataset is 25.\n",
    "\n",
    "We want to run our distribution-fitting experiment on HPS tally data, so we need to convert our FAPS tally data to pseudo-HPS tally data. We can do this by first expanding the FAPS data (using the constant expansion factor of 25), then compressing the expanded tally data using the reciprocal of the HPS expansion factor.\n",
    "\n",
    "First, we index the FAPS data by cover type and species group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.reset_index().set_index(['groupe3', 'type_couv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#cover_types = df.index.levels[1].unique()\n",
    "cover_types = ['r', 'm', 'f']\n",
    "species_groups = df.index.levels[0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to compile FAPS inventory data into 2cm wide bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def compile_hist(data, bins=None, xminmm=90, xmaxmm=610, xwmm=20, normed=True):\n",
    "    if bins is None:\n",
    "        bins = np.arange(xminmm, xmaxmm+xwmm, xwmm)\n",
    "    bin_vals, bin_edges = np.histogram(data, bins=bins, range=(xminmm, xmaxmm))\n",
    "    bin_centers = (bin_edges[:-1] + (xwmm * 0.5)) * 0.1\n",
    "    if normed: bin_vals = bin_vals * pow(0.1 * xwmm * sum(bin_vals), -1)\n",
    "    return bin_centers, 1. * bin_vals, bin_edges * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot empirical diameter distributions from expanded FAPS data, by species group and cover type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "def pairs(a):\n",
    "    return [(v, w) for v, w in zip(a[:-1], a[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "def signchanges(ydata):\n",
    "    signs = np.sign([np.array(v - w for v, w in pairs(ydata))])\n",
    "    return signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": true,
     "read_only": true
    }
   },
   "outputs": [],
   "source": [
    "def optimal_bins(df, sg, ct, xwmm=20, max_flipflops=3):\n",
    "    xdata, ydata, _ = compile_hist(df.loc[sg].loc[ct], xwmm=xwmm)\n",
    "    # delete trailing empty bins\n",
    "    while not ydata[-1]: \n",
    "        xdata = np.delete(xdata, -1)\n",
    "        ydata = np.delete(ydata, -1)\n",
    "    # increase bin width until no empty bins\n",
    "    flipflops = 99\n",
    "    xwmm = max(1, xwmm - 1)\n",
    "    while np.count_nonzero(ydata) < len(ydata) or flipflops > max_flipflops:\n",
    "        xwmm += 1\n",
    "        xdata, ydata = compile_hist(df.loc[sg].loc[ct], xwmm=xwmm)\n",
    "    return xdata, ydata, xwmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function that plots side-by-side subfigures showing empirical diameter distribution (as histogram, 25 2-cm wide bins subdividing the interval between 10 and 60 cm DBH), result of control method (fit size-biased form distribution to HPS tally data), and result of test method (fit standard form of distribution to expanded HPS data, with points weighted by the reciprocal of corresponding expansion factor in the fitting algorithm). Best-fit distribution from test method is projected onto HPS tally space in the left subfigure, and best-fit distribution from the control method is projected onto expanded HPS space in the right subfigure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick three combinations of species and cover type to use for the computational experiment (three different species, three different cover types, using combinations with large sample sizes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot use `functools.partial` to freeze some args before passing distribution functions to `curve_fit`, because it just sets arg values (like default values, the args are still in the signature so subject to parameter optimisation by the fitting algorithm). Instead, wrap distribution functions with a `lambda` function to freeze parameters and simplify the signature before fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to generate the subplots for our paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot(ax, fit_func1, fit_func2, bounds, xdata, ydata1, ydata2, bins,\n",
    "                 sigma=None, absolute_sigma=False, w=10, max_nfev=100000, \n",
    "                 xlim=None, ylim=None, alpha1=0.2, alpha2=0.3, \n",
    "                 linecolor='black', expand_data=False, bootstrap=False, use_popt=True):\n",
    "    \"\"\"Assume scipy.optimize leastsq wrapped to rename max_nfev to maxfev.\"\"\"\n",
    "    import warnings\n",
    "    __curve_fit = boots_fit if bootstrap else _curve_fit\n",
    "    m1 = Model(fit_func1)\n",
    "    popt1, pcov1 = __curve_fit(fit_func1, xdata, ydata1, sigma=None, bounds=bounds, max_nfev=max_nfev)\n",
    "    for i, pn in enumerate(m1.param_names):\n",
    "        vary = False if pn == 's' else True\n",
    "        value = popt1[i] if use_popt else 1.\n",
    "        m1.set_param_hint(pn, value=value, min=0., vary=vary)\n",
    "    p1 = m1.make_params()\n",
    "    fit_kws1 = {'sigma':None, 'bounds':bounds, 'max_nfev':max_nfev}\n",
    "    #with warnings.catch_warnings():\n",
    "    #    warnings.simplefilter('ignore')\n",
    "    #    mr1 = m1.fit(ydata1, x=xdata, **fit_kws1)\n",
    "    #mr1 = m1.fit(ydata1, x=xdata)\n",
    "    mr1 = m1.fit(ydata1, x=xdata, **fit_kws1)\n",
    "    m2 = Model(fit_func2)\n",
    "    popt2, pcov2 = __curve_fit(fit_func2, xdata, ydata2, sigma=sigma, absolute_sigma=absolute_sigma, bounds=bounds, max_nfev=max_nfev)\n",
    "    for i, pn in enumerate(m2.param_names):\n",
    "        vary = False if pn == 's' else True\n",
    "        value = popt2[i] if use_popt else 1.\n",
    "        m2.set_param_hint(pn, value=value, min=0., vary=vary)\n",
    "    p2 = m2.make_params()\n",
    "    fit_kws2 = {'sigma':sigma, 'absolute_sigma':absolute_sigma, 'bounds':bounds, 'max_nfev':max_nfev}\n",
    "    #with warnings.catch_warnings():\n",
    "    #    warnings.simplefilter('ignore')\n",
    "    #    mr2 = m2.fit(ydata2, x=xdata, fit_kws=fit_kws2)\n",
    "    mr2 = m2.fit(ydata2, x=xdata, **fit_kws2)\n",
    "    _ydata1 = mr1.best_fit\n",
    "    _ydata1 = _ydata1 / sum(_ydata1)\n",
    "    _ydata1_proj = transform_data(xdata, _ydata1, expansion_factor, normed=True)\n",
    "    _ydata2 = mr2.best_fit\n",
    "    _ydata2 = _ydata2 / sum(_ydata2)\n",
    "    _ydata2_proj = transform_data(xdata, _ydata2, compression_factor, normed=True)\n",
    "    __ydata1 = _ydata1 if not expand_data else _ydata1_proj\n",
    "    __ydata2 = _ydata2_proj if not expand_data else _ydata2\n",
    "    weights = ydata1 / sum(ydata1) if not expand_data else ydata2 / sum(ydata2)\n",
    "    linestyle1, linestyle2 = ('-', '--') if not expand_data else ('--', '-')\n",
    "    label1, label2 = ('Control', 'Test (proj.)') if not expand_data else ('Control (proj.)', 'Test')\n",
    "    ax.plot(xdata, ydata2*2., marker='o', markersize=4, linestyle='', color='k', alpha=0.5, label=r'$\\hat{y}_i$')\n",
    "    ax.plot(xdata, __ydata1, linestyle1, color=linecolor, alpha=alpha2, label=label1)\n",
    "    ax.plot(xdata, __ydata2, linestyle2, color=linecolor, alpha=alpha2, label=label2)\n",
    "    ax.legend(prop={'size':6})\n",
    "    if xlim: plt.xlim(*xlim)\n",
    "    if ylim: plt.ylim(*ylim)\n",
    "    return mr1, mr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "essencegroupe1 = pd.read_csv(data_path+'misc/essencegroupe1.csv', index_col='groupe1')\n",
    "groupe1groupe2groupe3 = pd.read_csv(data_path+'misc/groupe1groupe2groupe3.csv', index_col='groupe1')\n",
    "groupe1 = sorted(groupe1groupe2groupe3.index.unique())\n",
    "groupe2 = sorted(groupe1groupe2groupe3.groupe2.unique())\n",
    "groupe3 = sorted(groupe1groupe2groupe3.groupe3.unique())\n",
    "\n",
    "species_groups = groupe1groupe2groupe3.reset_index().set_index('groupe3')['groupe1'].to_dict()\n",
    "species_group_names = {'auf':'Other Hardwoods',\n",
    "                       'boj':'Yellow Birch',\n",
    "                       'bop':'Birch',\n",
    "                       'chce':'Oak-Hickory',\n",
    "                       'ers':'Maple',\n",
    "                       'erx':'Other Maples',\n",
    "                       'peu':'Poplar',\n",
    "                       'pib':'White Pine',\n",
    "                       'pir':'Red Pine',\n",
    "                       'sepm':'SPFL', #'Fir-Spruce-Pine-Larch',\n",
    "                       'topu':'Cedar'}\n",
    "cover_type_names = {'r':'Softwood', \n",
    "                    'm':'Mixedwood',\n",
    "                    'f':'Hardwood'}\n",
    "cover_type_names_short = {'r':'S', 'm':'M', 'f':'H'}\n",
    "dist_names_long = {'gg':'GG',\n",
    "                   'ib1':'IB1',\n",
    "                   'ga':'GA',\n",
    "                   'w':'W'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lmfit/model.py:951: UserWarning: The keyword argument sigma does notmatch any arguments of the model function.It will be ignored.\n",
      "  \"It will be ignored.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/lmfit/model.py:951: UserWarning: The keyword argument bounds does notmatch any arguments of the model function.It will be ignored.\n",
      "  \"It will be ignored.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/lmfit/model.py:951: UserWarning: The keyword argument max_nfev does notmatch any arguments of the model function.It will be ignored.\n",
      "  \"It will be ignored.\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/lmfit/model.py:951: UserWarning: The keyword argument absolute_sigma does notmatch any arguments of the model function.It will be ignored.\n",
      "  \"It will be ignored.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats.distributions as distributions\n",
    "bootstrap = False\n",
    "use_sigma = True\n",
    "absolute_sigma = False\n",
    "bounds = (0., np.inf)\n",
    "max_nfev = 10000\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(6, 7), sharey=True, sharex=True)\n",
    "distns = [('w', (D['sb2']['w'], D['st']['w'])), \n",
    "          ('ga', (D['sb2']['ga'], D['st']['ga']))]\n",
    "mplots = [('sepm', 'r'), ('bop', 'm'), ('ers', 'f')]\n",
    "compile_stderr = True\n",
    "plot_stderr = True\n",
    "ylim = (0., 0.5)\n",
    "expand_data = True\n",
    "results = {}\n",
    "out_flds = ['species',\n",
    "            'covertype',\n",
    "            'samplesize',\n",
    "            'distribution',\n",
    "            'chisqr1', 'chisqr2']\n",
    "out_data = {fld:[] for fld in out_flds}\n",
    "for i, v in enumerate(mplots):\n",
    "    sg, ct = v\n",
    "    results[v] = {}\n",
    "    for j, (dn, (fit_func1, fit_func2)) in enumerate(distns):\n",
    "        _df = df.loc[sg].loc[ct]\n",
    "        xdata, ydata2, bins, stderrs = compile_bin_data(_df, compile_stderr=compile_stderr)\n",
    "        xdata, ydata2, stderrs = zip(*[(x, y, stderr) for x, y, stderr in zip(xdata, ydata2, stderrs) if y > 0])\n",
    "        xdata = np.array(xdata)\n",
    "        #ydata1 = np.array(ydata1)\n",
    "        ydata2 = np.array(ydata2)\n",
    "        ydata1 = transform_data(xdata, ydata2, compression_factor)\n",
    "        _sigma = expansion_factor(xdata)\n",
    "        _sigma = _sigma / sum(_sigma)\n",
    "        sigma = _sigma if use_sigma else None\n",
    "        r = fit_and_plot(ax=ax[i][j],\n",
    "                         fit_func1=fit_func1, \n",
    "                         fit_func2=fit_func2,\n",
    "                         xdata=xdata, \n",
    "                         ydata1=ydata1, \n",
    "                         ydata2=ydata2, \n",
    "                         bins=bins,\n",
    "                         bounds=bounds,\n",
    "                         sigma=sigma,\n",
    "                         absolute_sigma=absolute_sigma,\n",
    "                         ylim=ylim,\n",
    "                         max_nfev=max_nfev,\n",
    "                         expand_data=expand_data)\n",
    "        mr1, mr2 = r\n",
    "        results[v][dn] = r\n",
    "        sgs = species_group_names[sg]\n",
    "        cts = cover_type_names_short[ct]\n",
    "        dns = dist_names_long[dn]\n",
    "        ax[i][j].set_ylabel('Meta-plot %s, %s' % (sg, ct))\n",
    "        ax[i][0].set_ylabel('Relative Frequency')\n",
    "        ax[i][j].set_xlabel('Meta-plot: %s-%s, Distribution: %s' % (sgs, cts, dns))\n",
    "        sample_size = len(_df)\n",
    "        out_data['species'].append(sgs)\n",
    "        out_data['covertype'].append(cts)\n",
    "        out_data['distribution'].append(dns)\n",
    "        out_data['samplesize'].append(sample_size)\n",
    "        out_data['chisqr1'].append(mr1.chisqr)\n",
    "        out_data['chisqr2'].append(mr2.chisqr)        \n",
    "plt.tight_layout()\n",
    "#plt.savefig(data_path + 'dbhdistfit_method-hpsdata.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out results in $\\LaTeX$ tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame(out_data)[out_flds]\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "900px",
   "left": "0px",
   "right": "1708px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
